##### netty 
并发高 传输快 封装好 

BIO 等待客户端发数据的过程是阻塞的，一个线程只能处理一个请求
socket 建立好之后将其交给 selector,后者遍历请求所有的socket,一旦

https://www.jianshu.com/p/b9f3f6a16911
页缓存和socket缓冲区

nc baidu.com 80 

netstat -natp 

唯一的连接 socket 连接就是 ip:port+目标ip:目标port
一个socket 最多只能建立 65535 个链接


ip的七层协议 4层协议的作用
nio 和 bio 的区别是什么


###### NIO 
socket 可以支持阻塞和非阻塞
accept 属于系统调用 x80 软中断 空循环问题，需要减少系统调用
多路复用和事件驱动
select
poll
epoll

系统不需要 receive 结果，则是 aio
同步io 模型

channel 可读可写 buffer position capacity limit



1、NIO存在的问题：

NIO的API比较复杂，需要熟练掌握3个核心组件，channel、buffer和selector；
需要熟悉多线程、网络编程等技术；
开发工作量大，难度也比较大，需要解决断连、重连、网络闪断、半包读写、失败缓存、网络拥堵等各种情况；
NIO存在bug，一个叫Epoll的bug，会导致选择器空轮询，形成死循环，最后CPU飙到100%
正是因为NIO存在这些问题，netty就应运而生了。


netty是一个异步的，基于事件驱动的网络应用框架。可以快速地开发高性能的服务器端和客户端，像dubbo和elasticsearch底层都用了netty。
它具有以下优点：

    设计优雅，灵活可扩展；
    使用方便，用户指南清晰明确；
    安全，完整的SSL/TLS和StartTLS支持；
    社区活跃，不断地更新完善
1、线程模型：
目前存在的线程模式：传统阻塞IO的服务模型和Reactor模式

根据Reactor的数量和处理资源的线程数不同，又分3种：
1 单Reactor单线程；
2 单Reactor多线程；
3 主从Reactor多线程

Netty的线程模型是基于主从Reactor多线程做了改进。

2、传统阻塞IO的线程模型：
采用阻塞IO获取输入的数据，每个连接都需要独立的线程来处理逻辑。存在的问题就是，当并发数很大时，就需要创建很多的线程，占用大量的资源。连接创建后，如果当前线程没有数据可读，该线程将会阻塞在读数据的方法上，造成线程资源浪费。

3、Reactor模式(分发者模式/反应器模式/通知者模式)：
针对传统阻塞IO的模型，做了以下两点改进：

    基于IO复用模型：多个客户端共用一个阻塞对象，而不是每个客户端都对应一个阻塞对象
    基于线程池复用线程资源：使用了线程池，而不是每来一个客户端就创建一个线程

Reactor模式的核心组成：

    Reactor：Reactor就是多个客户端共用的那一个阻塞对象，它单独起一个线程运行，负责监听和分发事件，将请求分发给适当的处理程序来进行处理
    Handler：处理程序要完成的实际事件，也就是真正执行业务逻辑的程序，它是非阻塞的

I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），
能够通知程序进行相应的读写操作。因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，
而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。
异步io是写 buffer,内核通知应用程序处理完毕，可以进行操作
同步异步IO区别在于用户程序是否需要自己copy数据

select 通过 fd_set 传递文件描述符进行查询状态 read accept
select最大的缺陷就是单个进程所打开的FD是有一定限制的，它由FD_SETSIZE设置，默认值是1024,64位是2048
线性扫描结果，效率比较低
poll 通过 poll_fd 传递文件描述符,
没有最大连接数，文件没有状态变更时则挂起，直到就绪或者超时才进行再次遍历。大量的fd数组在内核和用户空间进行复制，比较浪费
epoll 没有最大连接数限制，不是采用轮询方式，而是采用回调的方式通知。内核开辟一块空间存储文件描述符，减少了复制操作，使用mmap 方式进行内存 copy


epoll_create 创建内核空间
epoll_ctl 
epoll_wait



jsp -l 
cd /proc/进程号/许多线程号

strace -ff out 


网络IO 
![](./images/123344455555321.png)